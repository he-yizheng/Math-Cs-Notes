\documentclass{tufte-handout}

%\geometry{showframe}
%\geometry{showframe}% for debugging purposes -- displays the margins

\usepackage{amsmath}
\usepackage{cleveref}
\crefname{Proposition}{Proposition}{Propositions}
\Crefname{Proposition}{Proposition}{Propositions}
\crefname{Theorem}{Theorem}{Theorems}
\Crefname{Theorem}{Theorem}{Theorems}
\crefname{Definition}{Definition}{Definitions}
\Crefname{Definition}{Definition}{Definitions}
\crefname{Corollary}{Corollary}{Corollaries}
\Crefname{Corollary}{Corollary}{Corollaries}
\crefname{Lemma}{Lemma}{Lemmas}
\Crefname{Lemma}{Lemma}{Lemmas}
\crefname{Example}{Example}{Examples}
\Crefname{Example}{Example}{Examples}
\usepackage{amsthm}

% Set up the images/graphics package
\usepackage{graphicx}
\setkeys{Gin}{width=\linewidth,totalheight=\textheight,keepaspectratio}
\graphicspath{{graphics/}}

% The following package makes prettier tables.  We're all about the bling!
\usepackage{booktabs}

% The units package provides nice, non-stacked fractions and better spacing
% for units.
\usepackage{units}

% The fancyvrb package lets us customize the formatting of verbatim
% environments.  We use a slightly smaller font.
\usepackage{fancyvrb}
\fvset{fontsize=\normalsize}

% Small sections of multiple columns
\usepackage{multicol}

% Provides paragraphs of dummy text
\usepackage{lipsum}

% Defines colors
\usepackage{xcolor}
\definecolor{blue}{cmyk}{0.63, 0.37, 0, 0.57}     
\definecolor{ltblue}{RGB}{78,150,179}

% These commands are used to pretty-print LaTeX commands
\newcommand{\doccmd}[1]{\texttt{\textbackslash#1}}% command name -- adds backslash automatically
\newcommand{\docopt}[1]{\ensuremath{\langle}\textrm{\textit{#1}}\ensuremath{\rangle}}% optional command argument
\newcommand{\docarg}[1]{\textrm{\textit{#1}}}% (required) command argument
\newenvironment{docspec}{\begin{quote}\noindent}{\end{quote}}% command specification environment
\newcommand{\docenv}[1]{\textsf{#1}}% environment name
\newcommand{\docpkg}[1]{\texttt{#1}}% package name
\newcommand{\doccls}[1]{\texttt{#1}}% document class name
\newcommand{\docclsopt}[1]{\texttt{#1}}% document class option name

% Package for title style
\usepackage{sectsty}
\usepackage[utf8]{inputenc}

% Sets section number style
\setcounter{secnumdepth}{3} % uncomment this, if desired
\renewcommand\thesection{\color{white}\arabic{section}}

% Sets title style
\makeatletter
  \renewcommand{\paragraph}{\@startsection{paragraph}%
    {4}{\z@}{-1ex \@plus -1ex \@minus -.3ex}%
    {0.5ex \@plus .2ex}{\normalfont\normalsize\bfseries}}
\makeatother

\makeatletter
  \renewcommand{\subsection}{\@startsection{subsection}%
    {3}{-1.8em}{-3ex \@plus -1ex \@minus -.2ex}%
    {1.5ex \@plus .2ex}
    {\hspace*{-5.5em}\fcolorbox{ltblue}{ltblue}{\parbox[c][1.0ex][b]{4em}{\phantom{space}}}
    \normalfont\large\itshape\color{ltblue}}}
\makeatother

\makeatletter
  \renewcommand{\section}{\@startsection{section}%
    {3}{-1.01em}{-3ex \@plus -1ex \@minus -.2ex}%
    {1.5ex \@plus .2ex}
    {\hspace*{-5.5em}\fcolorbox{blue}{blue}{\parbox[c][1.0ex][b]{4em}{\phantom{space}}}
    \normalfont\Large\itshape\color{blue}}}
\makeatother

% Sets theorem style
\usepackage{thmtools}
\usepackage{transparent}
\definecolor{theb}{rgb}{0.67, 0.80, 0.91}

\declaretheorem[shaded={bgcolor=Lavender,
    textwidth=30em}]{Definition} % Colorbox-styled Theorem 
\declaretheorem[shaded={bgcolor=Thistle,
    textwidth=30em}]{Theorem} % Colorbox-styled Theorem 
\declaretheorem[shaded={bgcolor=Thistle,
    textwidth=30em}]{Corollary} % Colorbox-styled Theorem
\declaretheorem[shaded={bgcolor=PeachPuff,
    textwidth=30em}]{Proposition} % Colorbox-styled Proposition
\declaretheorem[shaded={bgcolor=Thistle,
    textwidth=30em}]{Lemma} % Colorbox-styled Lemma
\declaretheorem[shaded={rulecolor=Lavender,
    rulewidth=2pt, bgcolor={rgb}{1,1,1}}]{Example-1} % Colorbounded-styled Theorem
\declaretheorem[thmbox=L]{boxtheorem L} % Theorem box L-size
\declaretheorem[thmbox=M]{Example} % Theorem box M-size
\declaretheorem[thmbox=S]{Formula} % Theorem box S-size
\declaretheorem[thmbox=S]{Statement} % Theorem box S-size

% set up pdf bookmark depth
\hypersetup{bookmarksdepth=3}

% -----------------------------------------------------------------------

\title{MATH323 Probability (Extended)}
\author[Matthew He]{Matthew He}
\date{December 11, 2024}
% if the \date{} command is left out, the current date will be used

% Beginning of the document
\begin{document}

\maketitle% this prints the handout title, author, and date

% abstract
\begin{abstract}
\noindent This is meant to be a quick survey of important concepts in introductory probability theory. 
It goes faster and more in-depth than elementary probability class,
but not as technical as those pure math courses, which might be 
good for people just getting into research like me.\\

\textbf{Reference textbook:} \\
\textit{Introduction to Probability} by David F. Anderson, Timo Seppäläinen, and Benedek Valkó (2017)\\
\textit{Mathematical Statistics with Applications }by Dennis Wackerly, William Mendenhall, and Richard L. Scheaffer (2007)\\
\textit{MATH447 Stochastic Process notes} by Jana Kurrek
\end{abstract}

% main text
\section{Experiments with random outcomes}

    \subsection{Ingredients of a Probability Model}
        \marginnote{    \vspace*{20ex}

            \textbf{Kolmogorov Axioms (early 1930s)}

        \begin{enumerate}
            \item $0 \leq P(A) \leq 1 $ for event A.
            \item $P(\Omega) = 1$ and $ P(\emptyset) =0$.
            \item If $ A_1, A_2 ......$ is a sequence of pairwise disjoint events \\
            ($E_i \cap E_j = \emptyset $) for $ i \neq j$, \\
            then \\ $ P(E_1 \cup E_2 \cup E_3 ......) = \sum_{i=1}^{\infty} P(E_i)$ \\
            or \[P(\bigcup_{i=1}^{\infty})=\sum_{i=1}^{\infty}P(A_{i})\]
        \end{enumerate}
        *Axiom 3 can also be stated in terms of finite union of events as\\
        $ P(E_1 \cup E_2 \cup E_3 ......) = \sum_{i=1}^{n}P(E_i)$\\
        *Axiom 3 states that we can calculate probability of an event by summing up probabilities of its
        disjoint decomposed events.}


        \begin{Definition}
            These are ingredients of a probability model. \\
            \begin{itemize}
                \item The \textbf{sample space} $\Omega$ is the set of all possible outcomes of
                the experiment. Elements of $ \Omega $ are called \textbf{sample points} 
                and typically denoted by $ \omega $.
                \item Subsets of $ \Omega $ are called \textbf{events}. 
                The collection of events in $ \Omega $ is denoted by $ \mathcal{F} $.
                \item The \textbf{probability measure} 
                (also called \textbf{probability distribution} or simply \textbf{probability})
                P is a function from $ \mathcal{F} $ into the real numbers. 
                Each event A has a probability of \textit{P(A)}, and
                P satisfies the axioms on the right. 
            \end{itemize}
            The triple $ (\Omega, \mathcal{F}, \mathit{P}) $ is called a \textbf{probablity space}. 
            Every mathematically precise model of a random experiment or collection of experiments must be 
            of this kind.

        \end{Definition}


    \subsection{Random Sampling}
        \begin{Theorem}[Random Sampling]
            Let S be a finite sample space with N equally likely events and let E be an 
            event in S. Then \newline 
            \begin{center}
                $P(E) = \frac{n}{N}$
            \end{center}
        \end{Theorem} 

        \marginnote{This important theorem can reduce the problem of finding probabilities 
        to a counting problem.}

        \paragraph{Counting Rule 1: Multiplication Rule}
            \textbf{Sampling with replacement, order matters.}
            Consider k sets, Set 1 and Set 2 ... Set k. Set 1 has $n_{1}$, Set 2 has $n_{2}$ ... Set k has $n_{k}$
            distinct objects. Then the number of ways to form a set by choosing one object from each set is $n_{1}n_{2}...n_{k}$.

        \paragraph{Counting Rule 2: Factorial Rule}
            \textbf{Sampling without replacement, order matters.}
            The number of ways to arrange \textit{n} distinct objects is $n!$. \\
            $0! = 1 $and $1! = 1$

        \paragraph{Counting Rule 3: Permutation Rule }
            \textbf{Sampling without replacement, order matters.}
            The number of ways to arrange \textit{r} chosen from \textit{n} distinct object at a time without 
            replacement, where the order matters, is known as \textbf{permutations} of \textit{n} objects taken \textit{r} at a time.\\
            It is given by: $^{n}P_{r}=\dfrac{n!}{(n-r)!}$

        \paragraph{Counting Rule 4: Combination Rule}
            \textbf{Sampling without replacement, order irrelevant.}
            The number of ways to select \textit{r} object from \textit{n} distinct total objects at a time without replacement,
            where order does not matter, is known as \textbf{combination} of \textit{n} objects taken \textit{r} at a time.
            It is given by: $\binom{n}{r} = \dfrac{n!}{(n-r)!r!}$
            \marginnote{$\binom{n}{r}$ is also called binomial coefficient.}
            
    \subsection{Consequences of the rules of probability}
        \paragraph{Decomposing an event}
        If $ A_{1}, A_{2},..... $ are pairwise disjoint events and A is their union, then \\
        $ P(A) = P(A_{1}) + P(A_{2}) + .... $. Calculation of the probability of a complicated event A 
        almost always involves decomposing A into smaller disjoint pieces whose probabilities are easier to find.
        Both finite and infinite decomposition is possible.
        \begin{Theorem}[Events and complements]\phantom{xx} \\
            For \it any event A \rm ,$ P(A)^c = 1 - P(A)$
        \end{Theorem}

        \begin{Theorem}
            $P(\emptyset) =0$
        \end{Theorem}

        \begin{Theorem} 
            $P(A\cup B^C) = P(A) - P(A \cap B^c)$
        \end{Theorem}

        \marginnote{\bf Proof: \rm \newline
            Express A as the union of disjoint events as
            $A = (A \cap B^C) \cup (A \cap B)$ \newline
            $P(A) = P(A \cap B^C) + P(A \cap B)$ \newline by Axiom 3, \newline
            $\Rightarrow P(A\cup B^C) = P(A) - P(A \cap B^c)$}

        \begin{Theorem}[Monotonicity of probability]
            If $A \subset B$ then $P(A) \leq P(B)$
        \end{Theorem}

        \noindent  \bf Proof: \rm \newline
        $B = A \cup (A^C \cap B)$, $P(B)=P(A) + P(A^C \cap B)$ -- Axiom 3 \newline
        As $P(A^C \cap B) \geq 0$ -- Axiom 1, $\Rightarrow P(B) \geq P(A) or P(A) \leq P(B)$

        \begin{Theorem}[Inclusion-exclusion formulas]
            \[P(A\cup B) = P(A) + P(B) - P(A \cap B)\]
            \begin{align*}
            P(A\cup B\cup C ) = P(A) + P(B)+P(C) - P(A\cap B) - P(A \cap C) \\
            - P(B \cap C) + P(A\cap B \cap C)
            \end{align*}
            General Formula:
            \begin{flalign*}
                \begin{split}
                P(A_{1}\cup ... \cup A_{n}) = \sum_{i=1}^{n }P(A_{i }) - \sum_{1 \leq i_{1} < i_{2}\leq n }P(A_{i_{1}}\cap A_{i_{2}})\\
                +\sum_{1 \leq i_{1} < i_{2} < i_{3} \leq n }P(A_{i_{1}}\cap A_{i_{2}}\cap A_{i_{3}})\\
                -\sum_{1 \leq i_{1} < i_{2} < i_{3} \leq i_{4} \leq n }P(A_{i_{1}}\cap A_{i_{2}}\cap A_{i_{3}} \cap A_{i_{4}})\\
                + ... 
                +(-1)^{n+1}P(A_{i_{1}}\cap ... \cap A_{i_{n}}) \\
                = \sum_{k=1}^{n }(-1)^{k+1} \sum_{1\leq i_{1} < ... < i_{k } \leq n } P(A_{i_{1}}\cap ...\cap A_{i_{k}})
            \end{split} 
            \end{flalign*}
            
        \end{Theorem}

        \noindent \textbf{Proof:} \newline
            $A \cup B = (A \cap B^C) \cup (A \cap B) \cup (A^C \cap B)$ \newline
            $P(A \cup B) = P(A \cap B^C) + P(A \cap B) + P(A^C \cap B)$ \newline
            $P(A \cup B) = (P(A) - P(A \cap B)) + P(A \cap B) + (P(B) - P(A \cap B))$ \newline
            By Theorem 3, therefore $P(A \cup B) = P(A) + P(B) - P(A \cap B)$



        

    
        \noindent \textbf{Proof} : \newline
            Write E as the union of its simple events (elementary outcomes). \\
            $E = \cup ^{n} _{i = 1} E_{i}$ \\
            As the simple events are disjoint,\\
            \noindent  $P(E) = \sum_{n}^{i=1} P(E_{i})$ by Axiom 3.\\
            \noindent Similarly, $S = \cup^{N} _{i=1} E_{i}$ and $P(S) = \sum_{N}^{i=1} P(E_{i})$ by Axiom 3. \\
            Since all event $E_{i}$ are equally likely (have the same probability of occurrence) \\
            $\sum_{i=1}^{N}P(E_{i}) = NP(E_{i})$ also $P(S) =1$ by Axiom 2 \\
            Hence, $NP(E_{i}) = 1$ and $P(E_{i}) = \frac{1}{N} $\\
            Therefore, $P(E) = \sum_{i=1}^{n}P(E_{i}) = \sum_{i=1}^{n} \frac{1}{N} = \frac{n}{N}$ 


    \marginnote{\textit{\textbf{Side notes ssss}}}

    \subsection{Continuity of the probability measure}
        \begin{Theorem}
            Suppose we have an infinite sequence of events $ A_{1}, A_{2},\dots $ that are nested increasing:
            $ A_{1} \subset A_2 \subset \cdots \subset A_n \subset \cdots $. Let $ A_{\infty} = \cup_{k=1}^{\infty}A_k  $
            denote the union. Then
            \[\lim_{n\rightarrow\infty}P(A_n)=P(A_{\infty}).\]
        \end{Theorem}

        Another way to state this is as follows: if we have increasing events then 
        the probability of the union 
        of all the events(the probability that at least one of them happens) is equal to
         the limit of the individual probabilities.\\

        \marginnote{
        Recall from calculus that a function $ f: \mathbb{R} \rightarrow \mathbb{R } $ is continuous at x 
        if and only if for each sequence of point $ x_1, x_2, \dots $ that converge to x, we have 
        $ f(x_n) \rightarrow f(x) $ as $ x \rightarrow \infty $. In a natural way increasing sets $ A_n  $ converge to 
        their union $ A_{\infty} $, because the 
        difference $ A_{\infty} \backslash A_n $ shrinks away as n $ n \rightarrow \infty $.}

        \textit{Proof.} To take advantage of the additivity axiom of probability, we break up the events 
        $ A_n $ into disjoint pieces. For $ n=2,3,4,\dots $ let $ B_n = A_n \backslash A_{n-1} $.\\
        Now we have the disjoint decomposition of $ A_n  $ as 
        \[A_n=A_{n-1}\cup B_n = A_{n-2} \cup B_{n-1} \cup B_n = \cdots = A_1 \cup B_2 \cup \cdots \cup B_n.\]
        Taking union of all the events $ A_n $ gives us the disjoint decomposition
        \[\cup_{n=1}^{\infty}A_n=A_1 \cup B_2 \cup B_2 \cup B_3 \cup \cdots\]
        By the additivity of probability, and by expressing the infinite series as the limit of partial sums,
        \[P(\cup_{n=1}^{\infty}A_n)=\lim_{n \rightarrow \infty}(P(A_1)+P(B_2)+\cdot+P(B_{n}))=\lim_{n \rightarrow \infty}P(A_n).\]
        Q.E.D

    \subsection{Measurability}

        Every subset of a discrete sample space $ \Omega $ is a legitimate event. For example,
        the sample space of flipping a single coin is $ \Omega = \left\{H,T \right\}$ and the collection of 
        events is $ \mathcal{F} = \left\{\emptyset, \left\{H \right\}, \{T\}, \{H,T\}  \right\} $, which is 
        exactly the collection of all subsets of $ \Omega $, namely the power set of $ \Omega $.

        This all seems very straightforward. But there can be good reasons to use smaller collection $ \mathcal{F } $
         of events. It can be useful for modeling purposes, and solve the technical problems with uncountable sample spaces 
         preventing us from taking the $ \mathcal{F } $ as the power set.

         To put the theory on a sound footing, we extend the axiomatic framework to impose the following requirements
         on the collection of event $ \mathcal{F } $:

         \begin{Definition}[$ \sigma-algebra $] \
            Any collection $ \mathcal{F } $ of sets satisfying the following properties is call $ \sigma-algebra $ or $ \sigma-field$.
             \begin{enumerate}
                \item the empty set $ \emptyset $ is a member of $ \mathcal{F}$,
                \item if A is in $ \mathcal{F } $, then $ A^c  $ is also in $ \mathcal{F } $,
                \item if $ A_1, A_2, A_3, \cdots $ is a sequence of events in $ \mathcal{F }$, then 
                their union $ \cup_{i=1}^{\infty}A_i  $ is also in $ \mathcal{F } $.
            \end{enumerate}
         \end{Definition}

        The members of a $ \sigma-algebra $ are called \textit{measurable} sets.
        The properties of a $ \sigma-algebra $ imply that countably many applications of the usual
        set operations to events is a safe way to produce new events.

        Fortunately all reasonable sets and functions encountered in practice are measurable.

         Another aspect of the collection $ \mathcal{F } $ of events is that it can represent \textit{information}.



\section{Conditional probability and independence}
    \subsection{Condition probability}
        \marginnote{\textbf{Fact:}\\
        Let B be an event in the sample space $ \Omega $ such that $ P(B) > 0 $. 
        Then, as a function of the event A, the conditional probability $ P(A|B) $ satisfies the Kolmogorov Axioms.
        Especially, we have \\
        $ P(\cup_{i=1}^{\infty}B_{i }|A) = \sum_{i=1}^{\infty}P(B_{i }|A)$ \\
        where, $ B_{i } \cap B_{j } = \emptyset $ for $ i \neq j $}
        
        \begin{Definition}[Conditional probability] \phantom{text} \\
            Let B be an event in the sample space $ \Omega $ such that $ P(B) > 0 $. Then for all 
            events A the \textbf{conditional probability} of A given B is defined as 
            \[P(A|B )=\frac{P(AB )}{P(B )}\]
        \end{Definition}\
        
        \begin{Theorem}
            Suppose that we have an experiment with \textbf{finitely many equally likely outcomes} and B is not the empty set.
            Then, for any event A 
            \[P(A|B ) = \frac{\#AB }{\#B}\]
        \end{Theorem}

        \marginnote{Or simply, we have \[P(A\cap B) = P(B|A)P(A) = P(A|B)P(B)\] \\
        \textbf{Hints:}
            \begin{enumerate}
                \item If required to find $ P(A\cap B) $, look for either $ P(A ) $ or $ P(B ) $ and one 
                of the conditional probabilities.
                \item In word problems "of those that" implies a conditional probability.
                \item Do not confuse "and" with "given that"
            \end{enumerate}}
        

        \begin{Theorem}[Multiplication rule for n events]
            If $ A_{1}, ... , A_{n } $ are events and all the conditional probabilities below make sense then we 
            have
            \[P(A_{1}\cdot \cdot \cdot A_{n })=P(A_{1})P(A_{2}|A_{1})P(A_{3}|A_{2}A_{1})\cdot \cdot \cdot P(A_{n }|A_{1}\cdot \cdot \cdot A_{n-1}) \]\\
        \end{Theorem}
        
        \textbf{Nots:} This implies that problems involving the intersection of several events can be 
        simplified to a great extent by conditioning backwards.

        \paragraph{Three special cases of connditional probability}
        \begin{enumerate}
            \item Let A and B be two disjoint events, then, $ A \cap B = \emptyset $ and \\
                    $ P(B|A)=0 $, since $ P(A\cap B ) = 0 $ 
            \item Let A and B be two events, such that $ B \subset A  $. Then, \\
            $ P(B|A ) = \frac{P(A \cap B )}{P(A)} = \frac{P(B )}{P(A )} $ 
            \item Let A and B be two events, such that $ A \subset B  $. Then, \\
            $ P(B|A ) = \frac{P(A \cap B )}{P(A)} = \frac{P(A )}{P(A )} = 1 $ 
            
        \end{enumerate}

        \paragraph{Calculating probability by decomposition}
        For example, a general version of the reasoning can be: 
        \begin{flalign}
            P(A) = P(AB) + P(AB^{c })= P(A|B)P(B) +P(A|B^{c })P(B^{c }).
        \end{flalign}
        The idea is the decomposition of a complicated event A into disjoint pieces that are 
        easier to deal with. Above we used the pair $ \left\{ B, B^{c } \right\} $ to split A into two pieces. 
        $ \left\{ B, B^{c } \right\} $ is an example of a \textit{partition}.
        
        \begin{Definition}[Partition]
            A finite collection of event $ \left\{ B_{1}, \dots, B_{n } \right\} $ is a \textbf{partition}
            of $ \Omega $ if the sets $ B_{i } $ are pairwise disjoint and together they make up $ \Omega $. That is
            , $ B_{i }B_{ j} = \emptyset $ whenever $ i \neq  j$ and $ \cup_{i=1}^{n }B_{i }= \Omega $
        \end{Definition}
        
        \marginnote{This equation is true for the same reason as the eq. (1). \\
        Namely, set algebra gives 
        \[A = A \cap \Omega = A \cap \left( \bigcup_{i=1}^{n }B_{i } \right) = \bigcup_{i=1}^{n} A B_{i }\]
        \[P(A) = P(\bigcup_{i=1}^{n} A B_{i }) \]}
        

        \begin{Theorem}[The Law of Total Probability]
            Suppose that $ B_{1}, \dots , B_{n } $ is a partition of $ \Omega $ with $ P(B_{i }) >0$
            for $ i = 1, \dots, n $. Then for any event A we have 
            \[P(A) = \sum_{i=1}^{n }P(AB_{i }) = \sum_{i=1}^{n }P(A|B_{i })P(B_{i })\]
            More generally, $ P(A )=\mathbb{E }\left[ P(A|X ) \right] $.
        \end{Theorem}

        \marginnote{\textbf{Bayes' formula for two events.}\\
            For events A and B,
            \[P(A|B ) =\frac{P(AB)}{P(B)}= \frac{P(B|A)P(A)}{P(B )}\]}
        \marginnote{post = likehood * prior/marginization }
        
        \begin{Definition}[General Version of Bayes' Formula]
            Let $ B_{1}, \dots, B_{n } $ be a partition of the sample space $ \Omega $ such that
            each $ P(B_{i }) > 0 $. Then for any event A with $ P(A ) > 0 $, and any $ k = 1,\dots,n $.
            \[P(B_{k } |A ) =\frac{P(AB_{k})}{P(A)} = \frac{P(A|B_{k })P(B_{k })}{\sum_{j}(A|B_{j})P(B_{j})}\]
        \end{Definition}

\section{Random Variables}
    \subsection{A First Look}
    In addition to basic outcomes themselves, we are often 
    interested in various numerical values derived from the outcomes.
    \begin{Definition}[Random Variable]
        Let $ \Omega $ be a sample space. A \textbf{random variable }is a \textbf{function} 
        from $ \Omega $ into the real number.
    \end{Definition}
    \marginnote{\textbf{Some conventions:}\\
    Random variables, not variables but functions, are usually denoted by capital 
    letters such as X, Y and Z. The value of a random variable X at sample point $ \omega $ is $ X(\omega) $.}
    
    \begin{Definition}[Probability Distribution]
        Let X be a random variable. The \textbf{probability distribution} of the random 
        variable X is the collection of probabilities $ P(X\in B) $ for sets B of real numbers.
    \end{Definition}
    The probability distribution of a random variable is an assignment of probability to subsets of $ \mathbb{R } $
    that satisfies again the axioms of probability. 

    \marginnote{\vspace*{5ex} \newline That said, if the range of the random variable X is finite or countably infinite, then 
    X is a discrete variable. Those k for which $ P(X=k ) >0$ are the possible values of X.}
    
    \begin{Definition}[Discrete Random Variable]
        A random variable X is a \textbf{discrete random variable }is there exists a finite or countably 
        infinite set $ \left\{ k_1, k_2, k_3, \dots \right\} $ of real numbers such that 
        \[\sum_{i }^{}P(X=k_i )=1\]
        where the sum ranges over the entire set of points $\left\{ k_1, k_2, k_3, \dots \right\}   $.
    \end{Definition}

    \begin{Definition}[Continuous Random Variable]
        A random variable X with CDF \( F_X(x) \) is saed to be Continuous
        if \( F_X(x) \) is a continuous function for all \( x \in \mathbb{R} \).
    \end{Definition}
    
    \subsection{Different kinds of random variables}
\paragraph{Bernoulli random variable}
\marginnote{Examples: \\
\begin{itemize}
    \item Making application for a job
    \item Tossing a coin
    \item Getting tested for Covid-19
\end{itemize}}
A bernoulli random variable is related to the occurrence (or non-occurrence)
of a certain event E. If event E occurs, then X = 1; otherwise, X = 0.

\paragraph{Binomial random variable}
Before introducing the Binomial Distribution, we need to define the Binomial Experiment.
\begin{Definition}[Binomial experiment]
    A experiment is called a binomial experiment if it satisfies the following conditions:
    \begin{itemize}
        \item it consists of n independent Bernoulli trials
        \item the probability of success p remain constant from trial to trial
        \item We are interested in \( x \) successes out of \( n \) trials.\\
        Where \( x = 0,1,2,\ldots,n\).
    \end{itemize}
\end{Definition}
\marginnote{Bernoullil trial is a trial with two outcomes, success and failure.
A success is the outcome of interest. Let's denote the probability of success as \( p \).}

\begin{Definition}[Binomial random variable]
    Let X be the random variable that counts the number of successes in \( n \) Bernoulli
    trials, where the probability of success in each trial is \( p \). Then X is a 
    Binomial random variable with the parameters \( n \) and \( p \), and it's probability distribution is called the Binomial distribution.
    We say \( X \sim B(n,p) \).
\end{Definition}

\begin{Theorem}
    For a binomial random variable \( X \sim B(n,p) \), the probability mass function is given by
    \[ P(X = x) = \binom{n}{x} p^x (1-p)^{n-x} \]
\end{Theorem}
\marginnote{\( x \) measures the number of successes in \( n \) independent Bernoulli trials.}

\paragraph{Geometric random variable}
Sometimes we are interested in the number of trials needed to get the first success.
\begin{Definition}[Geometric random variable]
    A random variable \( X \) is said to have a geometric distribution with parameter \( p \) if
    it's probability mass function is given by
    \[ P(X = x) = (1-p)^{x-1} p \]
    where \( x = 1,2,3,\ldots \), and \( 0 < p < 1 \).
\end{Definition}
\marginnote{The random variable \( X \) is the number of trials at
 which the first success occurs.}

\begin{Theorem}
    Let X be a random variable with a geometric distribution with parameter \( p \).
    Then
    \[P(X=x) = (1-p)^{x-1}p,\]
    where \( x = 1,2,3,\ldots \) and \( 0 < p < 1 \).
\end{Theorem}

Note that
\begin{itemize}
    \item The Binomial random variable gives the number of successes in the fixed number of trials.
    \item The Geometric random variable gives the number of trials at which the first success occurs,
    where the number of trials is not fixed.
\end{itemize}

\paragraph{Negative Binomial random variable}
\begin{Definition}[Negative binomial random variable]
    The negative binomial random variable X gives the trial on which
    the \( r \)th success occurs in a sequence of independent Bernoulli trials.
    Each trial has two possible outcomes, success and failure. THe probability
    of success remains constant from trial to trial.
\end{Definition}

\begin{Theorem}
    Let X be negative binomial random variable, then
    \[ P(X = x) = \binom{x-1}{r-1} p^r (1-p)^{x-r} \]
    where \( x = r, r+1, r+2, \ldots \) and \( 0 < p < 1 \).
\end{Theorem}

\paragraph{Poisson random variable}
\begin{Definition}[Poisson random variable]
    A random variable \( X \) is said to have a Poisson distribution with parameter \( \lambda \)
    if it's probability mass function is given by
    \[ P(X = x) = \frac{e^{-\lambda} \lambda^x}{x!} \]
    where \( x = 0,1,2,\ldots \) and \( \lambda > 0 \).
\end{Definition}

\begin{Theorem}
    Let \( X \sim \text{Binom}(n,p)\), where \( n \rightarrow \infty \) and \( p \rightarrow 0 \)
    and \( np = \lambda \) (constant). Then
    \[ P(X = x) = \lim_{n \rightarrow \infty} \binom{n}{x} p^x (1-p)^{n-x} = \frac{e^{-\lambda} \lambda^x}{x!} \]
\end{Theorem}

\text{Proof.}
\begin{align}
        \lim_{n \rightarrow \infty} P(X = x) &= \lim_{n \rightarrow \infty} \binom{n}{x} p^x (1-p)^{n-x} \\
        &= \lim_{n \rightarrow \infty} \binom{n}{x} (\frac{\lambda}{n})^x (1-\frac{\lambda}{n})^{n-x} \\
    \end{align}
For \( x = 0,1,2,\ldots \) and \( \lambda > 0 \).

\paragraph{Hypergeometric random variable}
    \begin{Definition}[Hypergeometric random variable]
        The random variable \( X \) is a hypergeometric random variable with parameters
        A hypergeometric random variable $X$ represents the number of successes in $n$ draws without replacement from a finite population of size $N$ that contains exactly $K$ successes. It models situations where sampling is done without replacement, and each draw changes the probabilities of subsequent draws.

        The probability mass function is:

        \[ P(X = k) = \frac{\binom{K}{k} \binom{N - K}{n - k}}{\binom{N}{n}} \]

        where $k = 0, 1, 2, \ldots, n$ and $0 \leq k \leq \min(n, K)$.
    \end{Definition}

    \subsection{Probability Distributions of Random Variables}

    \marginnote{\vspace*{5ex}\\The function $ p_X  $ gives the probability of each possible value of X. Probabilities of
    other events of X then come by additivity: for any subset $ B \subset \mathbb{R} $
    \[P(X\in B ) = \sum_{k\in B }P(X=k) = \sum_{k\in B }p_X(k)\]}

    \begin{Definition}[Probability Mass Function]
        The \textbf{probability mass function }(p.m.f) of a discrete random variable X 
        is the function p (or $ p_X  $) defined by
        \[p(k)=P(X=k)\]
        for possible values k of X.
    \end{Definition}

    \marginnote{\vspace*{5ex}\\In fact, if f satisfies this definition, then 
    \[P(X\in B ) = \int_{B}f(x)dx\]
    for any subset B of the real line for which integration makes sense.}

    \begin{Definition}[Probability Density Function]
        Let X be a random variable. If a function f satisfies 
        \[P(X \leq b ) = \int_{-\infty}^{b}f(x)dx\]
        for all real values b, then f is the \textbf{probability density function}(p.d.f) of X.
    \end{Definition}

    
    \begin{Theorem}
        If a random variable X has density function f then point values have probability zero:
        \[P(X=c) = \int_{c }^{c }f(x)dx = 0 \qquad \text{for any real c}\]
    \end{Theorem}
    It follows that a random variable with a density function is not discrete, and the 
    probabilities of interval are not changed by including or excluding endpoints.

    \textbf{Remark.} A random variable X can not have two different density functions.

    \subsection{Cumulative Distribution Function}
    \begin{Definition}[Cumulative Distribution Function]
        The cumulative distribution function (c.d.f) of a random variable X is defined by
        \[F(s) = P(X \leq s) \qquad \text{for all } s \in \mathbb{R}\]
    \end{Definition}
    \marginnote{Be mindful of the convention that the inequality is $ \leq $ in the equation.}
    
    The cumulative distribution function gives a way to describe the probability distribution of any 
    random variable, including those that do not fall into the discrete or continuous categories.
    The cumulative distribution function give probabilities of left-open right-closed intervals of the form (a,b]:
    \[P(a < X \leq b) = P(X \leq b) - P(X \leq a) = F(b) - F(a)\]

    Note that:
    \begin{itemize}
        \item the domain of the CDF is the real line \( (-\infty, +\infty) \) with the range \([0,1]\).
        \item CDF is a non decreasing function, that is, \( F(x) \leq F(y) \) for \( x \leq y \).
        \item The CDF is right continuous. It does not jump at x when you approach x from above (\( \lim_{x \to a^+} F(x) = F(a) \)).

    \end{itemize}
    \marginnote{However, contrary to discrete random variables,
    the CDF of a continuous random variable is a continuous function (there are no jumps).
    }
    
    Knowing these probabilities is enough to determine the distribution of X completely.

    \paragraph{Cumulative distribution function of a discrete random variable}
    \begin{equation}
    F(s) = P(X \leq s) = \sum_{k:k\leq s} P(X=k)
    \end{equation}
    where the sum extends over those possible values k of X that are less than or equal to s.

    \paragraph{Cumulative distribution function of a continuous random variable}
    \marginnote{It is important to notice the dy here. This is a dummy variable of integration.
                Conventionally, we do this to avoid confusion with the random variable X.}
    
    \begin{equation}
        F(s) = P(X \leq s) = \int_{-\infty}^{s }f_X(y)dy
    \end{equation}
    This equation comes from the definition of probability density function.

    \begin{Theorem}
        Let the random variable X have cumulative distribution function F.\\
        \begin{enumerate}
            \item Suppose F is piecewise constant. Then X is a discrete random variable.
            The possible values of X are the locations where F has jumps, and if x is such a point, then 
            P(X = x) equals the magnitude of the jump of F at X. 
            \item Suppose F is continuous and the derivative $ F'(x) $ exists everywhere on the real line,
            except possibly at finitely many points. \\ Then X is continuous random variable and f(x) = F'(x) is the density function of X.
            If F is not differentiable at x, then the value f(x) can be set arbitrarily.
        \end{enumerate}
    \end{Theorem}


    \subsection{Expectation and variance}
    \paragraph{Expectation of a discrete random variable}
    \begin{Definition}
        The expectation or mean of a discrete random variable X is defined by
        \[E(X) = \sum_k k P(X=k)\]
        where the sum ranges over all the possible values k of X.
    \end{Definition}
    The expectation is also called the first moment, conventionally denoted as $ \mu =E(X) $.
    The expectation is the weighted average of the possible outcomes, where the weights are given by probabilities.

    \paragraph{Expectation of a continuous random variable}
    In continuous case averaging is naturally done via integrals. The weighting is given by the density function.
    
    \marginnote{\vspace*{7ex}\\It is important to keep separate the random variable (X on the left) and the integration variable(x on the right).}
    
    \begin{Definition}
        The expectation or mean of a continuous random variable X with density function f is 
        \[E[X]=\int_{-\infty}^{\infty} x f(x) dx\]
        An alternative symbol is $ \mu = E[X] $.
    \end{Definition}

    \paragraph{Variance of a continuous random variable}
    \marginnote{Properties of expectation and variance:\\
    \begin{flalign*}
        &E[a] = a  \text{ for any constant a} &\\
        &E[aX+bY] = aE[X]+bE[Y] &\\
        &\text{Var}(aX+b) = a^2\text{Var}(X) &
    \end{flalign*}}

    \begin{Definition}
        The variance of a continuous random variable X is:
        \[E(X-\mu)^2 = \int_{-\infty}^{\infty}(x-\mu)^2 f(x)dx\]
        or
        \[Var(X) = E(X^2) - [E(X)]^2 = \int_{-\infty}^{\infty}x^2f(x)dx- [E(X)]^2\]
    \end{Definition}

    \paragraph{Expectation of a function of a random variable}
    Taking a function of an existing random variable creates a new random variable.
    \begin{Theorem}
        Let g be a real-valued function defined on the range of a random variable X. If X is a discrete random 
        variable then 
        \[E[g(X)]=\sum_k g(k) P(X=k)\]
        while if X is a continuous random variable with density function f then
        \[E[g(X)]=\int_{-\infty}^{\infty}g(x)f(x)dx\]
    \end{Theorem}

    \textit{Proof.} The key is that the event {g(X)=y} is the disjoint union of the 
    events {X=k} over those values k that satisfy g(k)=y:
    \begin{align*}
        E[g(X)] &= \sum_y y P(g(X)=y) = \sum_y y \sum_{k:g(k)=y}P(X=k)\\
        &= \sum_y \sum_{k:g(k)=y} y P(X=k) = \sum_y \sum_{k:g(k)=y} g(k)P(X=k)\\
        &= \sum_k g(k) P(X=k)
    \end{align*}

    \begin{Theorem}
        The n \textbf{th moment} of the random variable X is the expectation $ E(X^n) $. 
        In the discrete case the nth moment is calculated by 
        \[E(X^n)=\sum_k k^n P(X=k)\]
        If X has density functino f its nth moment is given by
        \[E(X^n)=\int_{-\infty}^{\infty}x^n f(x) dx\]
    \end{Theorem}
    \marginnote{The second moment, $ E(X^2) $, is also called the mean square.}
    

    
    
    \begin{Theorem}
        The n th moment about the mean of a continuous random variable X is
        \[E(X-\mu)^n = \int_{-\infty}^{\infty}(x-\mu)^n f(x)dx\]
    \end{Theorem}

\subsection{Special continuous distributions}
\paragraph{Continuous uniform distribution}

\begin{Definition}[Continuous uniform distribution]
    A random variable \( X \) is said to have a continuous uniform distribution on the interval
    \([a,b]\), shown as \( X \sim \text{Uniform}(a,b) \),
    if it's probability density function is given by
    \[ f_X(x) = \begin{cases}
        \frac{1}{b-a} & \text{for } a \leq x \leq b \\
        0 & \text{otherwise}
    \end{cases} \]
\end{Definition}

\marginnote{The continuous uniform distribution with a = 0 and b = 1 is called \textbf{the standard uniform distributio}n.}

The support of continuous random variables is the set of all numbers whose
probability density function is positive. For the continuous uniform distribution \( X \sim \text{Uniform}(a,b) \),
the support is the interval [a,b].

\textbf{Properties of the continuous uniform distribution:}
\marginnote{\textit{Proof.}\\

\begin{align*}
    E(X) &= \int_{a}^{b} x \frac{1}{b-a} dx = \frac{1}{b-a}\left|x^2\slash 2\right|_{a}^{b} = \frac{a+b}{2}\\
    E(X^2) &=  \int_{a}^{b} x^2 \frac{1}{b-a} dx = \frac{1}{b-a}\left|x^3\slash 3\right|_{a}^{b}\\
    &= \frac{1}{3}(b^2+ab+a^2)\\
    Var(X) &= \frac{1}{3}(b^2+ab+a^2) - \frac{1}{4}(a+b)^2 = \frac{(b-a)^2}{12}\\
    F_X(x) &= \int_{-\infty}^{x} \frac{1}{b-a} dy\\
    &= \int_{-\infty}^{a} \frac{1}{b-a} dy +\int_{a}^{x} \frac{1}{b-a}a dy\\
    &= 0 + \frac{x-a}{b-a}\quad  \text{a < x < b}
\end{align*}
}

\begin{itemize}
    \item \( E(X) = \frac{a+b}{2} \)
    \item \( Var(X) = \frac{(b-a)^2}{12} \)
\end{itemize}
CDF of a continuous uniform distribution is given by

\[ F_X(x) = \begin{cases}
    0 & \text{for } x < a \\
    \frac{x-a}{b-a} & \text{for } a \leq x \leq b \\
    1 & \text{for } x > b
\end{cases} \]


\paragraph{Gamma distribution}
\begin{Definition}[Gamma function]
    The gamma function is defined by
    \[ \Gamma(\alpha) = \int_{0}^{\infty} x^{\alpha - 1} e^{-x} dx \]
\end{Definition}

\marginnote{Three properties of the gamma function are:\\
\begin{enumerate}
    \item \( \Gamma(\alpha) = (\alpha - 1) \Gamma(\alpha - 1) \)
    \item If n is a positive integer, then\\
    \( \Gamma(n) = (n-1)! \)
    \item \( \Gamma(1\slash 2) = \sqrt{\pi} \)
\end{enumerate}}


\begin{Definition}[Gamma distribution]
    A random variable \( X \) is said to have a gamma distribution with parameters \( \alpha > 0\) and \( \beta > 0\)
    if it's probability density function is given by
    \[ f_X(x) = \begin{cases}
        \frac{\beta^{\alpha} x^{\alpha - 1} e^{-\beta x}}{\Gamma(\alpha)} & \text{for } x > 0 \\
        0 & \text{otherwise}
    \end{cases} \]
\end{Definition}

\marginnote{\begin{align*}
    E(X) &= \int_{0}^{+\infty}xf(x) dx\\
    &= \int_{0}^{+\infty}x \frac{ x^{\alpha - 1} e^{-x \slash \beta}}{\Gamma(\alpha)\beta^{\alpha}} dx\\
    &= \frac{1}{\Gamma(\alpha)\beta^{\alpha}} \int_{0}^{+\infty} x^{\alpha} e^{-x \slash \beta} dx\\
    \text{Let } y = x \slash \beta &\text{, then }  dx \slash \beta =  dy\\
    \int_{0}^{+\infty}xf(x) dx &= \frac{1}{\Gamma(\alpha)\beta^{\alpha}} \int_{0}^{\infty} (\beta y)^{\alpha} e^{-y} \beta dy\\
    &= \frac{1}{\Gamma(\alpha)}\beta \int_{0}^{\infty} y^{\alpha} e^{-y} dy\\
    &= \frac{1}{\Gamma(\alpha)}\beta \Gamma(\alpha + 1)\\
    &= \alpha \beta \frac{1}{\Gamma(\alpha)}\Gamma(\alpha)\\
\end{align*}}

The expectation is \( E(X) = \alpha \beta \) and the variance is \( Var(X) = \alpha \beta^2 \).

\paragraph{Chi-square distribution}
Let X follows a gamma distribution with \( \alpha = v/2 \) and \( \beta = 1/2 \), where \( v \) is a
positive integer. Then, in this special case, X is said to have a chi-square distribution with \( v \) degrees of freedom.
\begin{Definition}[Chi-square distribution]
    A random variable \( X \) is said to have a chi-square distribution with \( n \) degrees of freedom 
    (\( X \sim \chi^2(v) \)), if it's probability density function is given by
    \[ f_X(x) = \begin{cases}
        \frac{1}{2^{v/2} \Gamma(v/2)} x^{v/2 - 1} e^{-x/2} & \text{for } x > 0 \\
        0 & \text{otherwise}
    \end{cases} \]
\end{Definition}

\marginnote{The chi-square distribution is typically used to develop hypothesis tests
and confidence intervals, and rarely for modeling real-world data.}

The properties of the chi-square distribution are:
\begin{itemize}
    \item \( E(X) = \alpha \beta = v \slash 2 \times 2 = v \)
    \item \( Var(X) = \alpha \beta^2 = v \times 2^2 = 2v \)
\end{itemize}

\paragraph{Normal distribution}
\begin{Definition}[Normal distribution]
    A random variable \( X \) is said to have a normal distribution with parameters \( \mu \) and \( \sigma^2 \),
    shown as \( X \sim N(\mu, \sigma^2) \), if it's probability density function is given by
    \[ f_X(x) = \frac{1}{\sqrt{2\pi} \sigma} e^{-\frac{(x-\mu)^2}{2\sigma^2}} \]
\end{Definition}
\marginnote{Standard normal distribution is a special case if \( \mu = 0 \) and \( \sigma = 1 \).}

\paragraph{Exponential distribution}
Consider the Gamma density function with shape parameter \( \alpha = 1 \) and scale parameter \( \beta > 0\). 
Then the random variable X is said to have an exponential distribution.
\begin{Definition}[Exponential distribution]
    A random variable \( X \) is said to have an exponential distribution with parameter \( \beta \),
    shown as \( X \sim \text{Exp}(\beta) \), if it's probability density function is given by
    \[ f_X(x) = \begin{cases}
        \frac{1}{\beta} e^{-\frac{1}{\beta} x} & \text{for } x > 0 \\
        0 & \text{otherwise}
    \end{cases} \]
\end{Definition}
\marginnote{The properties of the exponential distribution are:
\begin{itemize}
    \item \( E(X) = \beta \)
    \item \( Var(X) = \beta^2 \)
    \item Memoryless property: \( P(X > s+t | X > t) = P(X > s) \)
\end{itemize}}
The CDF of the exponential distribution is given by
\[ F_X(x) = \begin{cases}
    \int_{0}^{x}\frac{1}{\beta}e^{-y\slash \beta} dy = 1 - e^{-\frac{x}{\beta}} & \text{for } x > 0 \\
    0 & \text{for } x \leq 0
\end{cases} \]

\subsection{Moment generating functions}
\begin{Definition}
    The moment-generating function (MGF) of the (distribution of the) random variable X is
    the function of a real parameter t defined by
    \[ M_X(t) = E[e^{tX}], \]
    for all \( t \in \mathbb{R} \) for with the expectation \( E[e^{tX}] \) is welled defined.
\end{Definition}

\paragraph{Moment generating function of a discrete random variable}
\begin{equation}
    M_X(t) = E[e^{tX}] = \sum_{all x} e^{tk} P(X = x)
\end{equation}
\paragraph{Moment generating function of a continuous random variable}
\begin{equation}
    M_X(t) = E[e^{tX}] = \int_{-\infty}^{\infty} e^{tx} f_X(x) dx
\end{equation}

The PDF (or PMF) of a random variable X can be obtained from its moment generating function and vice versa.

\section{Multivariate probability distributions}
A multivariate probability distribution describes the joint behavior or two or more random variables.
\begin{Definition}[Joint probability function]
    Let X and Y be discrete random variables. The joint probability function of X and Y is the function
    $ p_{X,Y} $ defined by
    \[p_{X,Y}(x,y)=P(X=x,Y=y)\]
    for all possible values x and y of X and Y.
\end{Definition}

If X and Y are discrete random variables, then we have:
\begin{itemize}
    \item \( p_{X,Y}(x,y) \geq 0 \) for all x and y
    \item \( \sum_x \sum_y p_{X,Y}(x,y) = 1 \)
\end{itemize}
Once the joint PMF is determined, it becomes 
straight forward to compute the probability of any event involving X and Y.

\begin{Definition}[Joint cumulative distribution function]
    Let X and Y be random variables. The joint cumulative distribution function of X and Y is the function
    $ F_{X,Y} $ defined by
    \[F_{X,Y}(x,y)=P(X\leq x, Y\leq y)\]
    for all possible values x and y of X and Y.
\end{Definition}

If X and Y are jointly discrete random variables, then we have:
\begin{equation}
    F_{X,Y}(x,y) = P(X\leq x, Y\leq y) =
    \sum_{u\leq x} \sum_{v\leq y} p_{X,Y}(u,v)
\end{equation}
where, \( P_{X,Y}(u,v) \) is the joint PMF of X and Y.

Two random variables X and Y are jointly continuous if there exists a continuous function \( f_{X,Y} \) such that
\begin{equation}
    F_{X,Y}(x,y) = P(X\leq x, Y\leq y) = \int_{-\infty}^{x} \int_{-\infty}^{y} f_{X,Y}(u,v) du dv
\end{equation}
Generally,
\begin{align*}
    &P(X,Y)\in A = \int \int_A f_{X,Y}(u,v) du dv\quad \text{or,}\\
    &P(a_1 \leq X \leq a_2, b_1 \leq Y \leq b_2) = \int_{a_1}^{a_2} \int_{b_1}^{b_2} f_{X,Y}(u,v) du dv
\end{align*}

\subsection{Marginal probability distributions}
\begin{Definition}[Marginal probability mass function]
    Let X and Y be discrete random variables with joint probability mass function $ p_{X,Y} $. The marginal probability mass function of X is the function $ p_X $ defined by
    \[p_X(x)=P(X=x)=\sum_y P(X=x,Y=y)=\sum_y p_{X,Y}(x,y)\]
    for all possible values x of X.
\end{Definition}

\begin{Definition}[Marginal probability density function]
    Let X and Y be continuous random variables with joint probability density function $ f_{X,Y} $. The marginal probability density function of X is the function $ f_X $ defined by
    \[f_X(x)=\int_{-\infty}^{\infty} f_{X,Y}(x,y) dy\]
    for all possible values x of X.
\end{Definition}

\begin{Definition}[Marginal cumulative distribution function]
    Let X and Y be random variables with joint cumulative distribution function $ F_{X,Y} $. The marginal cumulative distribution function of X is the function $ F_X $ defined by
    \begin{align*}
    F_X(x)&=P(X\leq x)=P(X\leq x, Y\leq \infty)=F_{X,Y}(x,\infty)\\
    &= \lim_{y \to \infty} F_{X,Y}(x,y) \quad \text{for any possible values x of X.}
    \end{align*}
\end{Definition}
\marginnote{\begin{align*}
        \lim_{x,y \to \infty} F_{X,Y}(x,y) &= 1\\
        \lim_{x,y \to -\infty} F_{X,Y}(x,y) &= 0
    \end{align*}
}

To obtain PDF we can differentiate the CDF.
\begin{Definition}[Joint probability density function]
    Let X and Y be continuous random variables. The joint probability density function of X and Y is the function
    $ f_{X,Y} $ defined by
    \[f_{X,Y}(x,y)=\frac{\partial^2}{\partial x \partial y} F_{X,Y}(x,y)\]
    for all possible values x and y of X and Y.
\end{Definition}

To obtain PMF from CDF we can differentiate the CDF with respect to x and y.
But this becomes more complicated in higher dimensions.

\begin{Theorem}
    Let \( g(Y_1,Y_2)\) be some function of two random variables \( Y_1 \) and \( Y_2 \). Then the
    expectation of \( g(Y_1,Y_2) \) is
    \[ E[g(Y_1,Y_2)] = \sum_{y_1} \sum_{y_2} g(y_1,y_2) p_{Y_1,Y_2}(y_1,y_2) \]
    for discrete random variables and
    \[ E[g(Y_1,Y_2)] = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(y_1,y_2) f_{Y_1,Y_2}(y_1,y_2) dy_1 dy_2 \]
    for continuous random variables.
\end{Theorem}

\begin{Theorem}
    Let \( g(Y_1,Y_2)\) be some function of two random variables \( Y_1 \) and \( Y_2 \). Then the
    expected value of \( E(Y_i) \) is given by
    \[ E(Y_i) =  \int_{-\infty}^{\infty} y_i f_{Y_i}(y_i) dy_i .\]
    Generally, the expected value \( E(Y_i^k) \) is given by 
    \[ E(Y_i^k) =  \int_{-\infty}^{\infty} y_i^k f_{Y_i}(y_i) dy_i \quad \text{i = 1,2}.\]
\end{Theorem}

\section{Conditional Distributions and Expectation}

    \subsection{Conditioning on an event}
    First new definition comes by applying $ P(A|B) = \frac{P(AB)}{P(B)} $ to an event $ A = \left\{ X=k \right\} $ for
    a discrete random variable X.

    \begin{Definition}[Conditional probability mass function of X, given B]
        Let X be a discrete random variable and B an event with P(B)>0, Then the conditional
        probability mass function of X, given B is the function $ p_{X|B } $ defined as follows for all
        possible values k of X:
        \[p_{X|B}(k)=P(X=k|B)=\frac{P(\left\{ X = k \right\}\cap B)}{P(B)}.\]
    \end{Definition}
    \marginnote{Just like a regular probability mass function, its values are nonnegative and sum up to one.}
    
    The key point above was that the events $ \left\{ X = k \right\} \cap B$ are disjoint for different 
    values of k and their union over k is B.\\
    We can use the conditional probability mass function to compute an expectation.
    
    \begin{Definition}[Conditional expectation of X, given the event B]
        Let X be a discrete random variable and B an event with P(B)>0, Then the conditional
        expectation of X, given the event B is the function, is denoted by $ E[X|B ] $and defined as
        \[E[X|B]=\sum_k k p_{X|B }(k) = \sum_k k P(X=k|B)\]
        where the sum ranges over all possible values k of X.
    \end{Definition}
    
    Applying the averaging principle $ P(A)=\sum_{i=1}^{n}P(A|B_i)P(B_i) $ to an event $ A = \left\{ X=k \right\} $ gives
    the following identity:
    
    \begin{Theorem}
        Let $ \Omega $ be a sample space, X a discrete random variable on $ \Omega $, and $ B_1, \dots, B_n  $
        a partition of $ \Omega $ such that each $ P(B_i)>0 $. Then the (unconditional) 
        probabilities mass function of X can be calculated by averaging the conditional
        probabilities mass function:
        \[p_X(k) = \sum_{i=1}^{n }p_{X|B_i }P(B_i ).\]
    \end{Theorem}

    The averaging idea extends to expectations.
    \begin{Theorem}
        Let $ \Omega $ be a sample space, X a discrete random variable on $ \Omega $, and $ B_1, \dots, B_n  $
        a partition of $ \Omega $ such that each $ P(B_i )>0 $. Then
        \[E[X]=\sum_{i=1}^{n }E[X|B_i]P(B_i).\]
    \end{Theorem}



    \subsection{Conditioning on a random variable}
    Let the partition in "Conditioning on an event" part come from another discrete randomvariable Y, 
    then we followings.
    \begin{Definition}[Conditional Probability Mass Function]
        Let X and Y be discrete random variables. The conditional probability mass function of Y given X = x is
        the folloing two-variable function:
        \[p_{Y|X}(y|x)=P(Y=y|X=x)=\frac{P(Y=y,X=x )}{P(X=x)}=\frac{p_{Y,X}(y,x)}{p_X(x)}.\]
        The conditional expectation of Y given X = x is
        \[\mathbb{E }[Y|X=x ] = \sum_y y \cdot P(Y = y \mid X = x)=\sum_y y \cdot p_{Y|X}(y|x).\]
        The definition above are valid for y such that $P(X=x)>0$.
    \end{Definition}
    \marginnote{The conditional probability mass function $ p_{Y|X}(y|x) $ is just a probability mass function 
    in x for each fixed value of y, whenever $ p_X(x) >0 $.\\
    The conditional expectation also satisfies familiar properties of usual expectation. For example:
    \[\mathbb{E }[g(Y)|X=x ]=\sum_y g(y) \cdot p_{Y|X}(y|x)\]}

    \noindent  As y varies, the events {Y=y} form a partition of $ \Omega $. Hence, we have
    \begin{Theorem}
        Let X and Y be discrete random variables. Then
        \[p_X(x)=\sum_y p_{X|Y}(x|y)p_{Y}(y)\]
        and
        \[E(X) = \sum_y E[X|Y=y]p_Y(y).\]
        The sums extend over those values y such that $ p_Y(y)>0 $
    \end{Theorem}

    \subsection{Conditional distribution for jointly continuous random variables}

    \begin{Definition}[Conditional Probability Density Function]
        Let X and Y be jointly continuous random variables with joint density function $ f_{X,Y}(x,y).$
         The conditional probability density function of Y given X = x is,
        \[f_{Y|X }(y|x ) = \frac{f_{X,Y}(x,y)}{f_{X }(x)}\]
    \end{Definition}

    Just as an ordinary density function, a conditional one can also be used to calculated conditional 
    probabilities and expectations. The definition below gives the continuous counterpart of the discrete formula.

    \begin{Definition}
        The conditional probability that $ X \in A  $, given Y = y, is 
        \[P(X\in A|Y=y)=\int_A f_{X|Y}(x|y)dx.\]
        The conditional expectation of g(X), given Y = y, is
        \[E[g(X)|Y=y]=\int_{-\infty}^{\infty}g(x)f_{X|Y }(x|y)dx.\]
        The quantities above are defined for y such that $ f_Y(y)>0 $.
    \end{Definition}

    The averaging identities also works in the continuous case.

    \begin{Theorem}
        Let X and Y be jointly continuous. Then
        \[f_X(x) =\int_{\infty}^{\infty}f_{X|Y}(x|y)f_Y(y)dy. \]
        For any function g for which the expectations below make sense,
        \[E[g(X)]=\int_{\infty}^{\infty}E[g(X)|Y=y]f_Y(y)dy.\]
    \end{Theorem}

    \marginnote{\textbf{Summary of conditional probability} 
       \begin{itemize}
        \item Total Probability \[P(A) = \mathbb{E }[P(A|X )]\]
        \item Total Expectation \[\mathbb{E }[Y] = \mathbb{E }[\mathbb{E }[Y|X]]\]
        \item Total Conditional Expectation \[P(Y|A ) = \mathbb{E }[P(Y|X,A )|A]\]
        \item Total Conditional Probability \[\mathbb{E}[Y|A] = \mathbb{E}[\mathbb{E }[Y|X,A]|A]\]
    \end{itemize}
     }

    \subsection{Conditional expectation}
    In this section we discuss a conditional expectation that achieves some degree of unification of 
    treatment of discrete and continuous random variables. A quick recap:
    \begin{Definition}[Conditional Expectation]
        Let X and Y be discrete or jointly continuous random variables. 
        The conditional expectation of Y given X = x, denoted by $\mathbb{E }[Y|X=x ](x) $,
        is a function of x,
        \[\mathbb{E }[Y|X=x ](x) = 
        \begin{cases}
            \sum_y y \cdot P(Y = y \mid X = x) & \Omega \text{ is discrete} \\
            \int_{-\infty}^{\infty} y \cdot f_{Y|X}(y \mid x) dy & \Omega \text{ is continuous}
            \end{cases}\]
    \end{Definition}

    Before, we have the conditional expectation of X given Y = y, denoted by $ E[X|Y=y] $. \textbf{For each legitimate
    y-value, $ E[X|Y=y] $ is a real number. }We think of it as a function of y, denoted by $ v(y)=E[X|Y] $.
    We can summarize the construction also by saying that the random variable $ E(X|Y) $ takes the value $ E[X|Y=y] $ when Y = y.

    \marginnote{The key idea is that $ E[X|Y=y] $ is a real number, and it's a possible value of the function $ E(X|Y) $.}
    

    \begin{Definition}[Conditional expection as a random variable]
        Let X and Y be discrete or jointly continuous random variables. 
        The conditional expectation of X given Y, denoted by $ E(X|Y) $, is by definition the 
        random variable $ v(Y) $ where the function v is defined by $ v(y)=E(X|Y=y ) $.
    \end{Definition}

    \begin{Definition}
        The \textbf{conditional expectation} of Y  given A, for discrete case, is,
        \[E(Y|A ) = \frac{1}{P(A)}\sum_{y }yP(\left\{ Y=y \right\}\cap A)=\sum_{y }yP(Y=y|A)\]
    \end{Definition}

    \begin{Definition}[Law of Total Expectation]
        If $ A_{1},\dots,A_{k } $ partitions $ \Omega $ and Y is a random variable, 
        then the \textbf{law of total expectation } states that,
        \[\mathbb{E }\left[ Y\right]= \sum_{i=1}^{k }\mathbb{E }[Y|A_{i }]P(A_{i})
        \]
        More generally, $\mathbb{E }\left[ Y\right]= \mathbb{E}[\mathbb{E }[Y|X]]  $
    \end{Definition}
    \textit{Proof.} For the discrete case,
    \begin{align*}
        \mathbb{E}[\mathbb{E }[Y|X]] &= \sum_{x}\mathbb{E }[Y|X=x ] \cdot P(X=x ) \\
        &= \sum_{x} \left( \sum_y y \cdot P(Y = y \mid X = x) \right)P(X=x )\\
        &= \sum_y y \sum_x P(Y = y \mid X = x)\cdot P(X=x )\\
        &=\sum_y y \sum_x P(Y = y , X = x)\\
        &= \sum_y y \cdot P(Y=y)\\
        &= \mathbb{E }(Y)
    \end{align*}


    \subsection{Conditioning on multiple random variables}
    \marginnote{\textbf{A prelude to stochastic processes!}\\
                Finally we're about to get there.}
    
    A stochastic process in discrete time is a sequence of random variables $ X_0, X_1, X_2,\dots $.
    One can think of this sequence as the time evolution of a random quantity. The random variable $ X_n  $
    is called the state of the process at time n.\\

    
    \begin{align*}
        P(X_0=x_0, X_1=x_1,\dots,X_n=x_n)=P(X_0=x_0)P(X_1=x_1|X_0=x_0)\\
        \cdots P(X_n=x_n|X_0=x_0,\dots,X_{n-1}=x_{n-1})
    \end{align*}

    A larger important class of stochastic processes have the property that, at any given time, the past 
    influences the future only through the present state. Concretely speaking, all but the last state
    can be drop from the conditioning side of each conditional probability in the equation above. 

    \begin{Definition}[Markov Chain]
        Let $ X_0,X_1,X_2,\dots $ be a stochastic process oif discrete random variables. This 
        process is a Markov chain if 
        \[P(X_{n+1}=x_{n+1}|X_0=x_0,\dots,X_n=x_n)=P(X_{n+1}=x_{n+1}|X_n=x_n)\]
        for all $ n \geq 0 $ and all $ x_0,\dots,x_n  $ such that $P(X_0=x_0, X_1=x_1,\dots,X_n=x_n)$.
    \end{Definition}


\subsection{Independence}

\begin{Definition}
    If \( Y_1, Y_2 \) are discrete random variables with joint 
    probability mass function \( p_{Y_1,Y_2} \) and marginal probability mass functions
    \( p_{Y_1} \) and \( p_{Y_2} \), then \( Y_1 \) and \( Y_2 \) are independent if
    and only if
    \[ p_{Y_1,Y_2}(y_1,y_2) = p_{Y_1}(y_1)p_{Y_2}(y_2) \]
    for all pairs of real numbers \( y_1 \) and \( y_2 \).
\end{Definition}

\begin{Definition}
    If \( Y_1, Y_2 \) are continuous random variables with joint density function 
    \( f_{Y_1,Y_2} \) and marginal density functions \( f_{Y_1} \) and \( f_{Y_2} \), respectively.
    Then \( Y_1 \) and \( Y_2 \) are independent if and only if
    \[ f_{Y_1,Y_2}(y_1,y_2) = f_{Y_1}(y_1)f_{Y_2}(y_2) \]
    for all pairs of real numbers \( y_1 \) and \( y_2 \).
\end{Definition}

\subsection{Moment generating functions}
Moment generating functions are very important computational tools.
\begin{Theorem}
    Suppose that X and Y are independent random variables with moment generating functions
    \( M_X(t) \) and \( M_Y(t) \), respectively. Then for all real numbers t,
    \[ M_{X+Y}(t) = M_X(t)M_Y(t) \]
\end{Theorem}
\marginnote{This results can be helpful in finding the distribution of the 
sum of random variables, which can be extremely challenging otherwise.}
Note that the moment generating function of a random variable is unique.
So if we have the MGF, we can find its distribution.

\subsection{Covariance}
\begin{Definition}
    Let X and Y be random variables defined on the same sample space with the expectation
    \( E(X) = \mu_X \) and \( E(Y) = \mu_Y \). The covariance of X and Y is defined as
    \[ \text{Cov}(X,Y) = E[(X-\mu_X)(Y-\mu_Y)] \]
    if the expectation on the right is finite.
\end{Definition}

\begin{Theorem}
    The covariance of X and Y can also be calculated as
    \[ \text{Cov}(X,Y) = E(XY) - E(X)E(Y) \]
\end{Theorem}

\section{Tail bounds and limit theorems}

\subsection{Central limit theorem}
\begin{Theorem}[CLT - Independence]\hfill\\
    The random variables \( X_1, X_2, \dots, X_n \) are independent iff
    \begin{itemize}
        \item \(f_{X_1,X_2,\ldots,X_n}(x_1,x_2,\ldots,x_n) = f_{X_1}(x_1)f_{X_2}(x_2)\ldots f_{X_n}(x_n)\)\\
        (Continuous case)
        \item \(\text{Var}(\sum_{i=1}^{n}X_i) = \sum_{i=1}^{n}\text{Var}(X_i)\)\\
        \item \(M_{X_1+X_2+\ldots+X_n}(t) = M_{X_1}(t)M_{X_2}(t)\ldots M_{X_n}(t)\)\\
    \end{itemize}
\end{Theorem}
\begin{Theorem}[Central limit theorem]\hfill \\
    Suppose that we have independent and identically distributed random variables \( X_1, X_2, \dots \)
    with finite mean \( E[X_1] = \mu \) and finite variance \( Var(X_1) = \sigma^2 \).
    Let \( S_n = X_1 + X_2 + \dots + X_n \). Then for any fixed \( -\infty \leq a \leq b \leq \infty \)
    we have
    \[ \lim_{n \to \infty} P\left(a \leq \frac{S_n - n\mu}{\sigma \sqrt{n}} \leq b 
    \right) = \Phi(b) - \Phi(a) =
    \int_{a}^{b} \frac{1}{\sqrt{2\pi}} e^{-y^2/2} dy \]
    where \( \Phi \) is the standard normal distribution function.
\end{Theorem}
That said, we find the mean and variance of S as:
\begin{align*}
    E[S_n] &= n\mu \\
    Var(S_n) &= n\sigma^2
\end{align*}

\section{Time-Homogeneous Markov Chains}
    \subsection{Finite State, Time-Homogeneous Chains}
    
    \begin{Definition}[Finite State Stochastic Process]
        A \textbf{finite state stochastic process} $ (X_n)_{n \geq 0} $ has time steps in 
        $ \mathbb{N } $ and values in $ S = [N-1] $.
    \end{Definition}

    \begin{Definition}[Markov Property]
        The \textbf{Markov property } claims that for every 
        $ n \in \mathbb{N } $ and every sequence of states $ (i_0, i_1, \dots) $
        where $ i_j \in S  $, the behavior of a system depends only on the previous state,
        \[ P(X_n = i_n | X_0 = i_0, \dots, X_{n-1}=i_{n-1}) = P(X_n = i_n | X_{n-1} = i_{n-1})
        \]
    \end{Definition}

    \begin{Definition}[Time Homogeneity]
        A markov chain is \textbf{time-homogenous} if the probabilities
        in definition above do not depend on n,
        \[ P(X_n = i_n | X_{n-1} = i_{n-1}) = P(X_1 = i_1 | X_0 = i_0) \quad (n\in \mathbb{N})
        \]
    \end{Definition}

    \begin{Definition}[Transition Matrix]
        The \textbf{transition matrix P} for a time-homogeneous 
        Markov chain is the $ N \times N $ matrix whose $ (i,j)th $ entry 
        $ P_{ij} $ is the one-step transition probability $ p(i,j) = P(X_1 = j|X_0 = i) $
    \end{Definition}

    \marginnote{\textbf{Remark.} The transition matrix P 
    is stochastic, that is,
    \begin{itemize}
        \item (Non-Negative Entries) $ 0 \leq P_{ij} \leq 1$ for 
        $ 1 \leq i,j \leq N  $.
        \item (Row Sum Equal to 1) $ \sum_{j=1}^{N } P_{ij } = 1 $ for $ 1 \leq i \leq N  $.
    \end{itemize}}
    % \vspace*{3ex}
    \begin{Example}
        Let $(X_n)_{n \geq 0}$ denote a sequence of coin flips where,
            \[
            P(X_{n+1} = H \mid X_n) =
            \begin{cases}
            0.51 & \text{if } X_n = H \\
            0.49 & \text{if } X_n = T
            \end{cases}
            \]            and,
            \[
            P(X_{n+1} = T \mid X_n) =
            \begin{cases}
            0.51 & \text{if } X_n = T \\
            0.49 & \text{if } X_n = H
            \end{cases}
            \]            Then,            
            \[
            \mathbf{P} = 
            \begin{pmatrix}
            0.51 & 0.49 \\
            0.49 & 0.51
            \end{pmatrix}
            =
            \begin{pmatrix}
            P_{HH} & P_{HT} \\
            P_{TH} & P_{TT}
            \end{pmatrix}
            \]
    \end{Example}

    \subsection{Transition Probabilities }
    \begin{Definition}[Probability Distribution Vector]
        The \textbf{distribution }of a discrete random variable X is the vector $ \vec {\phi}$ if,
        \[\phi_j = P(X=j) \quad \forall j \in \mathbb{N }\]
    \end{Definition}

    \begin{Definition}[Initial Distribution Vector]
        The \textbf{initial probability distribution} of a Markov chain $ (X_n)_{n \geq 0}  $ 
        is the distribution $ \vec {\phi}$ of $ X_0 $.
    \end{Definition}

    \begin{Definition}[Transition Probabilities]
        The n-step transition probability $ p_n(i,j)=P(X_n = i |X_0 = j) $
         is the (i,j)th entry in the matrix $ \mathbf{P}^n $.
    \end{Definition}




% end of main text

\makeatletter
  \renewcommand{\section}{\@startsection{section}%
    {3}{0.8em}{-3ex \@plus -1ex \@minus -.2ex}%
    {1.5ex \@plus .2ex}
    {\hspace*{-5.5em}\fcolorbox{Periwinkle}{Periwinkle}{\parbox[c][1.0ex][b]{4em}{\phantom{space}}}
    \normalfont\Large\itshape\color{blue}}}
\makeatother

\bibliography{marginnotes}
\bibliographystyle{plainnat}

\end{document}